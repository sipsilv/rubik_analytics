{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9ef574",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m      2\u001b[0m required_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexchange\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexchange_token\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlot_size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m ]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Keep only existing columns\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m existing_columns \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m required_columns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Filter dataframe\u001b[39;00m\n\u001b[0;32m     20\u001b[0m final_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[:, existing_columns]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Required columns (DB schema)\n",
    "required_columns = [\n",
    "    \"exchange\",\n",
    "    \"exchange_token\",\n",
    "    \"trading_symbol\",\n",
    "    \"name\",\n",
    "    \"instrument_type\",\n",
    "    \"segment\",\n",
    "    \"series\",\n",
    "    \"isin\",\n",
    "    \"expiry_date\",\n",
    "    \"strike_price\",\n",
    "    \"lot_size\"\n",
    "]\n",
    "\n",
    "# Keep only existing columns\n",
    "existing_columns = [c for c in required_columns if c in df.columns]\n",
    "\n",
    "# Filter dataframe\n",
    "final_df = df.loc[:, existing_columns].copy()\n",
    "final_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29998858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables: [('schedulers',), ('symbols',), ('transformation_scripts',), ('upload_logs',)]\n",
      "Row count: 224610\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# DB PATH\n",
    "db_path = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\rubik-analytics\\data\\symbols\\symbols.duckdb\"\n",
    "\n",
    "# CONNECT\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "# CHECK TABLES\n",
    "tables = con.execute(\"SHOW TABLES\").fetchall()\n",
    "print(\"Tables:\", tables)\n",
    "\n",
    "# ROW COUNT (change table name if needed)\n",
    "row_count = con.execute(\"SELECT COUNT(*) FROM symbols\").fetchone()[0]\n",
    "print(\"Row count:\", row_count)\n",
    "\n",
    "# CLOSE\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed224a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column headers:\n",
      "id\n",
      "exchange\n",
      "trading_symbol\n",
      "exchange_token\n",
      "name\n",
      "instrument_type\n",
      "segment\n",
      "series\n",
      "isin\n",
      "expiry_date\n",
      "strike_price\n",
      "lot_size\n",
      "status\n",
      "source\n",
      "created_at\n",
      "updated_at\n",
      "last_updated_at\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# DB PATH\n",
    "db_path = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\rubik-analytics\\data\\symbols\\symbols.duckdb\"\n",
    "\n",
    "# CONNECT\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "# GET COLUMN HEADERS\n",
    "columns = con.execute(\"DESCRIBE symbols\").fetchall()\n",
    "\n",
    "print(\"Column headers:\")\n",
    "for col in columns:\n",
    "    print(col[0])\n",
    "\n",
    "# CLOSE\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dbd030b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionException",
     "evalue": "Connection Error: Connection already closed!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33msymbol_upload_logs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msymbols\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     count = \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT COUNT(*) FROM \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.fetchone()[\u001b[32m0\u001b[39m]\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mConnectionException\u001b[39m: Connection Error: Connection already closed!"
     ]
    }
   ],
   "source": [
    "for table in [\"symbol_upload_logs\", \"symbols\"]:\n",
    "    count = con.execute(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
    "    print(f\"{table}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8480e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "import sqlite3\n",
    "\n",
    "# Path to connections data\n",
    "base_path = Path(r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\rubik-analytics\")\n",
    "connections_file = base_path / \"data\" / \"connections\" / \"connections.json\"\n",
    "active_file = base_path / \"data\" / \"connections\" / \"active_connection.json\"\n",
    "\n",
    "# Load connections\n",
    "connections_data = {}\n",
    "active_connections = {}\n",
    "\n",
    "if connections_file.exists():\n",
    "    with open(connections_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        connections_data = {conn[\"id\"]: conn for conn in data.get(\"connections\", [])}\n",
    "\n",
    "if active_file.exists():\n",
    "    with open(active_file, 'r') as f:\n",
    "        active_connections = json.load(f)\n",
    "\n",
    "# Test connection status\n",
    "def test_connection(conn_type, config):\n",
    "    \"\"\"Test if a database connection is working\"\"\"\n",
    "    try:\n",
    "        if conn_type == \"sqlite\" or conn_type == \"sqlite3\":\n",
    "            path = config.get(\"path\") or config.get(\"database\")\n",
    "            if path and os.path.exists(path):\n",
    "                conn = sqlite3.connect(path)\n",
    "                conn.execute(\"SELECT 1\")\n",
    "                conn.close()\n",
    "                return \"✓ Connected\"\n",
    "            return \"✗ File not found\"\n",
    "        elif conn_type in [\"duckdb\", \"duckdb_direct\", \"duckdb_sqlalchemy\"]:\n",
    "            path = config.get(\"path\")\n",
    "            if path:\n",
    "                if os.path.exists(path):\n",
    "                    con = duckdb.connect(path)\n",
    "                    con.execute(\"SELECT 1\")\n",
    "                    con.close()\n",
    "                    return \"✓ Connected\"\n",
    "                return \"✗ File not found\"\n",
    "            # Multi-database case\n",
    "            return \"✓ Configured\"\n",
    "        else:\n",
    "            return \"? Unknown type\"\n",
    "    except Exception as e:\n",
    "        return f\"✗ Error: {str(e)[:30]}\"\n",
    "\n",
    "# Display connections\n",
    "print(\"=\" * 100)\n",
    "print(\"DATABASE CONNECTIONS STATUS\")\n",
    "print(\"=\" * 100)\n",
    "print()\n",
    "\n",
    "if connections_data:\n",
    "    # Create a list for display\n",
    "    conn_list = []\n",
    "    for conn_id, conn in connections_data.items():\n",
    "        category = conn.get(\"category\", \"N/A\")\n",
    "        is_active = conn_id in active_connections.values()\n",
    "        active_marker = \"✓ ACTIVE\" if is_active else \"  \"\n",
    "        \n",
    "        conn_type = conn.get(\"type\", \"N/A\")\n",
    "        name = conn.get(\"name\", \"N/A\")\n",
    "        \n",
    "        # Get database path(s)\n",
    "        config = conn.get(\"config\", {})\n",
    "        if \"path\" in config:\n",
    "            db_path = config[\"path\"]\n",
    "            # Test connection\n",
    "            conn_status = test_connection(conn_type, config)\n",
    "        elif conn_type == \"duckdb\" and any(k in config for k in [\"ohlcv\", \"indicators\", \"signals\", \"jobs\"]):\n",
    "            db_path = f\"Multiple: {', '.join([k for k in config.keys() if k != 'path'])}\"\n",
    "            conn_status = \"✓ Configured (multi-DB)\"\n",
    "        else:\n",
    "            db_path = \"N/A\"\n",
    "            conn_status = \"? Not configured\"\n",
    "        \n",
    "        conn_list.append({\n",
    "            \"Active\": active_marker,\n",
    "            \"Name\": name,\n",
    "            \"Type\": conn_type,\n",
    "            \"Category\": category,\n",
    "            \"Connection Status\": conn_status,\n",
    "            \"Database Path\": str(db_path)[:60] + \"...\" if len(str(db_path)) > 60 else str(db_path)\n",
    "        })\n",
    "    \n",
    "    # Display as DataFrame\n",
    "    df_connections = pd.DataFrame(conn_list)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', 70)\n",
    "    print(df_connections.to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Show active connections summary\n",
    "    print(\"-\" * 100)\n",
    "    print(\"ACTIVE CONNECTIONS BY CATEGORY:\")\n",
    "    print(\"-\" * 100)\n",
    "    for category, conn_id in sorted(active_connections.items()):\n",
    "        if conn_id:\n",
    "            conn = connections_data.get(conn_id, {})\n",
    "            config = conn.get(\"config\", {})\n",
    "            conn_type = conn.get(\"type\", \"N/A\")\n",
    "            status = test_connection(conn_type, config)\n",
    "            print(f\"  {category:20s} → {conn.get('name', conn_id):40s} ({conn_type:15s}) {status}\")\n",
    "        else:\n",
    "            print(f\"  {category:20s} → None\")\n",
    "    print()\n",
    "    \n",
    "    # Summary\n",
    "    active_count = sum(1 for cid in connections_data.keys() if cid in active_connections.values())\n",
    "    total_count = len(connections_data)\n",
    "    print(f\"Summary: {active_count} active out of {total_count} total connections\")\n",
    "else:\n",
    "    print(\"No connections found.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970b01ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jallu\\AppData\\Local\\Temp\\ipykernel_70964\\2181961351.py:3: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price             Close        High          Low         Open      Volume\n",
      "Ticker      RELIANCE.NS RELIANCE.NS  RELIANCE.NS  RELIANCE.NS RELIANCE.NS\n",
      "Date                                                                     \n",
      "2025-12-10  1536.900024      1547.5  1531.400024  1534.000000     7991629\n",
      "2025-12-11  1545.000000      1550.0  1524.000000  1536.900024     4706197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "df = yf.download(\n",
    "    \"RELIANCE.NS\",\n",
    "    start=\"2025-12-10\",\n",
    "    end=\"2025-12-12\",\n",
    "    interval=\"1d\"\n",
    ")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f88cf0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables: [('users',), ('access_requests',), ('feedback',), ('sessions',), ('feature_requests',), ('audit_logs',), ('sqlite_sequence',), ('symbols',), ('transformation_scripts',), ('symbol_upload_logs',), ('scheduled_ingestions',), ('ingestion_sources',), ('connections',)]\n",
      "Row count: 3\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# DB PATH\n",
    "db_path = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\rubik-analytics\\backend\\data\\auth\\sqlite\\auth.db\"\n",
    "\n",
    "# CONNECT\n",
    "con = sqlite3.connect(db_path)\n",
    "cur = con.cursor()\n",
    "\n",
    "# CHECK TABLES\n",
    "cur.execute(\"\"\"\n",
    "    SELECT name \n",
    "    FROM sqlite_master \n",
    "    WHERE type='table';\n",
    "\"\"\")\n",
    "tables = cur.fetchall()\n",
    "print(\"Tables:\", tables)\n",
    "\n",
    "# ROW COUNT (change table name if needed)\n",
    "table_name = \"users\"  # <-- replace with actual table name\n",
    "cur.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "row_count = cur.fetchone()[0]\n",
    "print(\"Row count:\", row_count)\n",
    "\n",
    "# CLOSE\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f39943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                username                   email        mobile  \\\n",
      "0   2  testuser@rubikview.com  testuser@rubikview.com    9515625875   \n",
      "1   4                 sandeep       sandeep@rubik.com    8686504620   \n",
      "2   5                   admin     admin@rubikview.com  +10000000000   \n",
      "\n",
      "                                     hashed_password         role  is_active  \\\n",
      "0  $2b$12$u7Sm8k/Rw6dstyT0P4lTnu9.gNJmE6lsZb08Ql9...        admin          1   \n",
      "1  $2b$12$FXa/jswKk5f1fmGg/hYNC.YAMz/Y8kdMpIzEoaa...  super_admin          1   \n",
      "2  $2b$12$CoWtm2TUU.8RhkxWsFhct.ISQbNrVflwPRhprmB...  super_admin          1   \n",
      "\n",
      "            created_at                  updated_at  \\\n",
      "0  2025-12-19 16:29:34  2025-12-23 19:47:00.676215   \n",
      "1  2025-12-20 12:24:46         2025-12-26 06:23:39   \n",
      "2  2025-12-21 06:56:55         2025-12-22 06:07:32   \n",
      "\n",
      "                    last_seen theme_preference           name  \\\n",
      "0  2025-12-20 08:28:11.304847             dark         testin   \n",
      "1  2025-12-26 06:15:38.376916             dark  Sandeep jallu   \n",
      "2  2025-12-22 06:07:32.443174             dark           None   \n",
      "\n",
      "                                user_id              last_active_at  \\\n",
      "0  faf3fc4a-23e2-4b55-80cb-0a38fc45acd0                        None   \n",
      "1                            7285504620  2025-12-26 06:23:39.635896   \n",
      "2  5395507f-77d7-451b-9d1e-8dab1b0a5057  2025-12-22 06:07:32.443202   \n",
      "\n",
      "  account_status  \n",
      "0         ACTIVE  \n",
      "1         ACTIVE  \n",
      "2         ACTIVE  \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# DB PATH\n",
    "db_path = r\"C:\\Users\\jallu\\OneDrive\\pgp\\Python\\Stock predictor\\rubik-analytics\\backend\\data\\auth\\sqlite\\auth.db\"\n",
    "\n",
    "def fetch_table_safe_df(table_name, limit=5):\n",
    "    con = sqlite3.connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # VALIDATE TABLE NAME\n",
    "    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    allowed = [t[0] for t in cur.fetchall()]\n",
    "\n",
    "    if table_name not in allowed:\n",
    "        con.close()\n",
    "        raise ValueError(\"Invalid table name\")\n",
    "\n",
    "    # FETCH FIRST N ROWS\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table_name}\n",
    "        ORDER BY rowid ASC\n",
    "        LIMIT {limit}\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(query, con)\n",
    "\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# USAGE\n",
    "# =========================\n",
    "df = fetch_table_safe_df(\"users\", limit=5)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffebadeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (4.13.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (2.3.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (5.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (from requests) (2025.6.15)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jallu\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abb57ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page title: Reliance Industries Ltd share price | About Reliance Industr | Key Insights - Screener\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.screener.in/company/RELIANCE/\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, timeout=15)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "print(\"Page title:\", soup.title.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d13ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 93 (3177863400.py, line 95)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 95\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif bs_df is not None:\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'if' statement on line 93\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "SYMBOL = \"AAA Technologies\"\n",
    "URL = f\"https://www.screener.in/company/{SYMBOL}/consolidated/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/131.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Referer\": \"https://www.screener.in/\"\n",
    "}\n",
    "\n",
    "def fetch_soup(url: str) -> BeautifulSoup:\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "def parse_header_fundamentals(soup: BeautifulSoup) -> dict:\n",
    "    \"\"\"\n",
    "    Extract Market Cap, Price, PE, ROE, etc. from the top info block.\n",
    "    This uses regex on full text to be more robust to minor HTML changes.\n",
    "    \"\"\"\n",
    "    txt = soup.get_text(\"\\n\")\n",
    "\n",
    "    def grab(pattern):\n",
    "        m = re.search(pattern, txt)\n",
    "        return m.group(1).strip() if m else None\n",
    "\n",
    "    data = {}\n",
    "    data[\"Market Cap (Cr)\"] = grab(r\"Market Cap\\s*₹\\s*([0-9,\\.]+)\\s*Cr\")\n",
    "    data[\"Current Price\"] = grab(r\"Current Price\\s*₹\\s*([0-9,\\.]+)\")\n",
    "    data[\"High / Low\"] = grab(r\"High\\s*/\\s*Low\\s*₹\\s*([0-9,\\. /]+)\")\n",
    "    data[\"Stock P/E\"] = grab(r\"Stock P/E\\s*([0-9\\.]+)\")\n",
    "    data[\"Book Value\"] = grab(r\"Book Value\\s*₹\\s*([0-9,\\.]+)\")\n",
    "    data[\"Dividend Yield %\"] = grab(r\"Dividend Yield\\s*([0-9\\.]+)\\s*%\")\n",
    "    data[\"ROCE %\"] = grab(r\"ROCE\\s*([0-9\\.]+)\\s*%\")\n",
    "    data[\"ROE %\"] = grab(r\"ROE\\s*([0-9\\.]+)\\s*%\")\n",
    "    data[\"Face Value\"] = grab(r\"Face Value\\s*₹\\s*([0-9,\\.]+)\")\n",
    "    return data\n",
    "\n",
    "def parse_peer_table(soup: BeautifulSoup) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Find the peer comparison table by looking for columns like 'CMP Rs.' and 'P/E'.\n",
    "    \"\"\"\n",
    "    tables = pd.read_html(str(soup))\n",
    "    for df in tables:\n",
    "        cols = [str(c) for c in df.columns]\n",
    "        joined = \" \".join(cols)\n",
    "        if \"CMP Rs.\" in joined and \"P/E\" in joined:\n",
    "            return df\n",
    "    return None\n",
    "\n",
    "def parse_section_table(soup: BeautifulSoup, heading_text: str) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    For blocks like 'Profit & Loss', 'Balance Sheet', 'Cash Flows', 'Ratios'.\n",
    "    \"\"\"\n",
    "    h = soup.find(lambda tag: tag.name in [\"h2\", \"h3\"] and heading_text in tag.get_text())\n",
    "    if not h:\n",
    "        return None\n",
    "    table = h.find_next(\"table\")\n",
    "    if not table:\n",
    "        return None\n",
    "    return pd.read_html(str(table))[0]\n",
    "\n",
    "def main():\n",
    "    soup = fetch_soup(URL)\n",
    "\n",
    "    # 1) Header fundamentals\n",
    "    header_data = parse_header_fundamentals(soup)\n",
    "    header_df = pd.DataFrame([header_data])\n",
    "    header_df.to_csv(f\"{SYMBOL}_header_fundamentals.csv\", index=False)\n",
    "    print(\"Header fundamentals:\")\n",
    "    print(header_df.T)\n",
    "\n",
    "    # 2) Peer comparison\n",
    "    peer_df = parse_peer_table(soup)\n",
    "    if peer_df is not None:\n",
    "        peer_df.to_csv(f\"{SYMBOL}_peers.csv\", index=False)\n",
    "        print(\"\\nPeer comparison (top 5):\")\n",
    "        print(peer_df.head())\n",
    "\n",
    "    # 3) Financial statements\n",
    "    pl_df = parse_section_table(soup, \"Profit & Loss\")\n",
    "    bs_df = parse_section_table(soup, \"Balance Sheet\")\n",
    "    cf_df = parse_section_table(soup, \"Cash Flows\")\n",
    "    ratio_df = parse_section_table(soup, \"Ratios\")\n",
    "\n",
    "    if pl_df is not None:\n",
    "        pl_df.to_csv(f\"{SYMBOL}_profit_loss.csv\", index=False)\n",
    "    if bs_df is not None:\n",
    "        bs_df.to_csv(f\"{SYMBOL}_balance_sheet.csv\", index=False)\n",
    "    if cf_df is not None:\n",
    "        cf_df.to_csv(f\"{SYMBOL}_cash_flows.csv\", index=False)\n",
    "    if ratio_df is not None:\n",
    "        ratio_df.to_csv(f\"{SYMBOL}_ratios.csv\", index=False)\n",
    "\n",
    "    print(\"\\nSaved CSVs for:\", SYMBOL)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
